{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4ec6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from fake_useragent import UserAgent\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import random\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c64a82",
   "metadata": {},
   "source": [
    "# Webscraping the abstracts from manuscripts.\n",
    "\n",
    "Here I'll load the list of publications, which includes the web address of each publication. Then I'll webscrape the abstracts from these publications and add them to the dataframe. The dataframe then gets saved for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8883e",
   "metadata": {},
   "source": [
    "## Load the list of publications and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29bfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the different sheets.\n",
    "\n",
    "yrs = [\"2019\", \"2020\"]\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for sheet in yrs:\n",
    "    temp_df = pd.read_excel(\"../data/2019 - 2020 JCRP Metrics -EB.xlsx\", \n",
    "                   sheet_name = sheet)\n",
    "    df = pd.concat([df, temp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc1d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some empty rows.\n",
    "\n",
    "df = df.dropna(axis=0, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188fc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of the Title entries are \"Selected Abstracts...\" and are from \"Literature Update\" article types.\n",
    "# However, the Vol 40 doesn't have article type so have to look through titles.\n",
    "\n",
    "df = df[df[\"Title\"].str.contains(\"Selected Abstracts From Recent Publications\") == False].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27008193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f4dac6b",
   "metadata": {},
   "source": [
    "## Webscrape the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c938680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to initialize the column for the full abstract.\n",
    "df[\"Full_Abstract\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331867c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "# Use random user agent when scraping.\n",
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "\n",
    "# Initialize a counter to keep track of progress.\n",
    "pbar = ProgressBar()\n",
    "\n",
    "for i, link in enumerate(pbar(df[\"Link\"])):\n",
    "    \n",
    "    # Ensure that there's a link.\n",
    "    if link == np.nan:\n",
    "        pass\n",
    "    else:\n",
    "        url = link\n",
    "\n",
    "        response = requests.get(url, headers = user_agent)\n",
    "        status = response.status_code\n",
    "        if status == 200:\n",
    "            page = response.text\n",
    "            soup = bs(page)\n",
    "        else:\n",
    "            print(f\"Oops! On df row {i} with this link ({url}),\\nreceived status code {status}\")\n",
    "            table = \"\"\n",
    "\n",
    "        # Add the abstract sections to the dataframe.\n",
    "        table = soup.find('div', attrs = {'id': 'article-abstract-content1'})\n",
    "\n",
    "        # If there's no abstract on the page, skip it.\n",
    "        if table:\n",
    "            temp_lst = []\n",
    "            for header in table.find_all(\"h3\"):\n",
    "                temp_lst.append(header.text[:-2])\n",
    "\n",
    "            # Add the individual sections as columns to dataframe.\n",
    "            # Then add full abstract.\n",
    "            full_paragraph = \"\"    \n",
    "            for j,paragraph in enumerate(table.find_all(\"p\")):\n",
    "                full_paragraph += paragraph.text + \" \"\n",
    "                \n",
    "                if len(temp_lst) != 0:\n",
    "                    df.loc[i,temp_lst[j]] = paragraph.text\n",
    "                    \n",
    "            df.loc[i,\"Full_Abstract\"] = full_paragraph[:-1]      \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Add (1.5-4.5sec) delay that makes the web scraping more human-like.\n",
    "    timeout = 1.5*random.randint(1,3)\n",
    "    time.sleep(timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54992a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list. \n",
    "with open('../data/dataframe_with_abstracts.pickle', 'wb') as to_write:\n",
    "    pickle.dump(df, to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e0a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test some random selections to make sure the abstracts are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8faad9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the 6-min Walk Test to Monitor Peak Oxygen Uptake Response to Cardiac Rehabilitation in Patients With Heart Failure\n",
      "https://journals.lww.com/jcrjournal/Fulltext/2020/11000/Using_the_6_min_Walk_Test_to_Monitor_Peak_Oxygen.6.aspx\n",
      "We examined the agreement between peak oxygen uptake (V˙o2peak), estimated using prediction equations from the 6-min Walk Test (6MWT), and V˙o2peak measured using a cardiopulmonary exercise test (CPX) to estimate change in V˙o2peak in patients with heart failure (HF) enrolled in cardiac rehabilitation (CR). This was secondary analysis of 54 (including 9 women) patients with HF who completed a clinical CR program. Four previously published equations using 6MWT distance were used to estimate V˙o2peak and were compared with a CPX at baseline, follow-up, and change using the standard and modified Bland-Altman method. Analyses were repeated for quartiles of cardiorespiratory fitness (CRF) based on measured V˙o2peak from the CPX. Bland-Altman plots revealed proportional bias between all prediction equations and the measured V˙o2peak. The difference between methods varied by the level of CRF, with overestimation of prediction equations at greater levels of CRF and underestimation at lower levels of CRF. This poor agreement remained when comparisons were made between the estimated and measured V˙o2peak values at quartiles of CRF, indicating prediction equations have limited ability to predict V˙o2peak at any level of CRF. Estimated V˙o2peak using 6MWT distance demonstrated poor agreement with measured V˙o2peak from a CPX. While distance ambulated on the 6MWT remains an important measure of physical performance in patients with HF, prediction equations using 6MWT distance are not appropriate to monitor changes in V˙o2peak following CR in patients with HF.\n"
     ]
    }
   ],
   "source": [
    "idx = random.randint(0,len(df))\n",
    "\n",
    "print(df.loc[idx,\"Title\"])\n",
    "print(df.loc[idx,\"Link\"])\n",
    "print(df.loc[idx,\"Full_Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf7f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
